{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import os\n",
    "from models import EncoderRNN, LuongAttnDecoderRNN\n",
    "from vocabulary import Voc\n",
    "import random\n",
    "from utils import loadPreparedData, batch2TrainData, train, validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed pairs\n",
    "training_pairs = loadPreparedData(\"data/training_pairs.txt\")\n",
    "validation_pairs = loadPreparedData(\"data/validation_pairs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct vocabulary\n",
    "with open(\"data/voc.txt\", \"r\") as f:\n",
    "    voc_dict = f.read()\n",
    "voc_dict = eval(voc_dict)\n",
    "voc = Voc(voc_dict['name'])\n",
    "voc.__dict__ = voc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters and hyperparameters\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "clip = 50.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "validate_print_save = n_iteration // 100\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers...\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "encoder = EncoderRNN(hidden_size=hidden_size, \n",
    "                     embedding=nn.Embedding(num_embeddings=voc.num_words, embedding_dim=hidden_size),\n",
    "                     n_layers=encoder_n_layers, \n",
    "                     dropout=dropout)\n",
    "\n",
    "decoder = LuongAttnDecoderRNN(attn_model='dot', \n",
    "                              embedding=nn.Embedding(num_embeddings=voc.num_words, embedding_dim=hidden_size), \n",
    "                              hidden_size=hidden_size, \n",
    "                              output_size=voc.num_words, \n",
    "                              n_layers=decoder_n_layers, \n",
    "                              dropout=dropout)\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 40; Train loss: 4601.6239; Val loss: 5.6065; Train perplexity: 556.0798; Val perplexity: 272.2034\n",
      "Iter: 80; Train loss: 3984.0238; Val loss: 5.6606; Train perplexity: 249.9569; Val perplexity: 287.3350\n",
      "Iter: 120; Train loss: 4042.6906; Val loss: 5.5603; Train perplexity: 240.9499; Val perplexity: 259.9080\n",
      "Iter: 160; Train loss: 4110.4025; Val loss: 5.5140; Train perplexity: 238.5346; Val perplexity: 248.1417\n",
      "Iter: 200; Train loss: 4005.0571; Val loss: 5.5206; Train perplexity: 226.1534; Val perplexity: 249.7895\n",
      "Iter: 240; Train loss: 3949.2191; Val loss: 5.4492; Train perplexity: 219.5968; Val perplexity: 232.5762\n",
      "Iter: 280; Train loss: 3809.3168; Val loss: 5.4466; Train perplexity: 199.1803; Val perplexity: 231.9658\n",
      "Iter: 320; Train loss: 4025.4761; Val loss: 5.4312; Train perplexity: 191.8540; Val perplexity: 228.4186\n",
      "Iter: 360; Train loss: 3879.2387; Val loss: 5.4230; Train perplexity: 185.8782; Val perplexity: 226.5617\n",
      "Iter: 400; Train loss: 3744.7536; Val loss: 5.4126; Train perplexity: 178.3215; Val perplexity: 224.2144\n",
      "Iter: 440; Train loss: 3794.5071; Val loss: 5.4390; Train perplexity: 163.8770; Val perplexity: 230.2048\n",
      "Iter: 480; Train loss: 3758.9893; Val loss: 5.4179; Train perplexity: 168.0700; Val perplexity: 225.3980\n",
      "Iter: 520; Train loss: 3859.4073; Val loss: 5.4212; Train perplexity: 183.7643; Val perplexity: 226.1514\n",
      "Iter: 560; Train loss: 3684.0456; Val loss: 5.4067; Train perplexity: 160.9779; Val perplexity: 222.8864\n",
      "Iter: 600; Train loss: 3780.6363; Val loss: 5.4213; Train perplexity: 155.2525; Val perplexity: 226.1713\n",
      "Iter: 640; Train loss: 3677.6948; Val loss: 5.3996; Train perplexity: 157.0620; Val perplexity: 221.3088\n",
      "Iter: 680; Train loss: 3664.6530; Val loss: 5.4237; Train perplexity: 146.4378; Val perplexity: 226.7188\n",
      "Iter: 720; Train loss: 3738.9505; Val loss: 5.3982; Train perplexity: 159.0546; Val perplexity: 221.0176\n",
      "Iter: 760; Train loss: 3500.8705; Val loss: 5.4598; Train perplexity: 130.1354; Val perplexity: 235.0467\n",
      "Iter: 800; Train loss: 3658.5343; Val loss: 5.4177; Train perplexity: 153.9080; Val perplexity: 225.3606\n",
      "Iter: 840; Train loss: 3729.7564; Val loss: 5.4426; Train perplexity: 148.4642; Val perplexity: 231.0475\n",
      "Iter: 880; Train loss: 3635.1949; Val loss: 5.3985; Train perplexity: 138.4138; Val perplexity: 221.0697\n",
      "Iter: 920; Train loss: 3636.4533; Val loss: 5.4496; Train perplexity: 139.5586; Val perplexity: 232.6739\n",
      "Iter: 960; Train loss: 3644.2653; Val loss: 5.4680; Train perplexity: 148.5436; Val perplexity: 236.9916\n",
      "Iter: 1000; Train loss: 3650.4298; Val loss: 5.4187; Train perplexity: 142.2568; Val perplexity: 225.5774\n",
      "Iter: 1040; Train loss: 3724.6387; Val loss: 5.4032; Train perplexity: 133.9272; Val perplexity: 222.1087\n",
      "Iter: 1080; Train loss: 3524.1162; Val loss: 5.4016; Train perplexity: 127.6312; Val perplexity: 221.7656\n",
      "Iter: 1120; Train loss: 3681.5782; Val loss: 5.3922; Train perplexity: 135.7614; Val perplexity: 219.6855\n",
      "Iter: 1160; Train loss: 3520.3860; Val loss: 5.4039; Train perplexity: 126.0098; Val perplexity: 222.2746\n",
      "Iter: 1200; Train loss: 3566.5952; Val loss: 5.4023; Train perplexity: 142.0021; Val perplexity: 221.9054\n",
      "Iter: 1240; Train loss: 3517.3954; Val loss: 5.4088; Train perplexity: 133.1152; Val perplexity: 223.3654\n",
      "Iter: 1280; Train loss: 3612.2081; Val loss: 5.3839; Train perplexity: 132.7056; Val perplexity: 217.8649\n",
      "Iter: 1320; Train loss: 3588.9018; Val loss: 5.4163; Train perplexity: 131.9857; Val perplexity: 225.0363\n",
      "Iter: 1360; Train loss: 3585.8870; Val loss: 5.4056; Train perplexity: 126.1689; Val perplexity: 222.6511\n",
      "Iter: 1400; Train loss: 3574.4796; Val loss: 5.4077; Train perplexity: 133.9842; Val perplexity: 223.1228\n",
      "Iter: 1440; Train loss: 3546.3787; Val loss: 5.4032; Train perplexity: 132.8468; Val perplexity: 222.1198\n",
      "Iter: 1480; Train loss: 3551.0402; Val loss: 5.4121; Train perplexity: 127.2958; Val perplexity: 224.1118\n",
      "Iter: 1520; Train loss: 3416.3608; Val loss: 5.4248; Train perplexity: 115.3053; Val perplexity: 226.9657\n",
      "Iter: 1560; Train loss: 3524.3873; Val loss: 5.3913; Train perplexity: 134.5534; Val perplexity: 219.4923\n",
      "Iter: 1600; Train loss: 3390.8363; Val loss: 5.4071; Train perplexity: 124.4702; Val perplexity: 222.9831\n",
      "Iter: 1640; Train loss: 3515.9635; Val loss: 5.3887; Train perplexity: 113.9345; Val perplexity: 218.9180\n",
      "Iter: 1680; Train loss: 3366.6947; Val loss: 5.3912; Train perplexity: 110.5759; Val perplexity: 219.4686\n",
      "Iter: 1720; Train loss: 3398.1041; Val loss: 5.4190; Train perplexity: 101.5230; Val perplexity: 225.6442\n",
      "Iter: 1760; Train loss: 3565.0129; Val loss: 5.4064; Train perplexity: 137.8867; Val perplexity: 222.8172\n",
      "Iter: 1800; Train loss: 3613.1476; Val loss: 5.3966; Train perplexity: 136.3436; Val perplexity: 220.6497\n",
      "Iter: 1840; Train loss: 3492.9238; Val loss: 5.4211; Train perplexity: 123.1837; Val perplexity: 226.1267\n",
      "Iter: 1880; Train loss: 3552.0842; Val loss: 5.4038; Train perplexity: 123.7602; Val perplexity: 222.2404\n",
      "Iter: 1920; Train loss: 3461.4331; Val loss: 5.4025; Train perplexity: 130.3357; Val perplexity: 221.9522\n",
      "Iter: 1960; Train loss: 3577.5386; Val loss: 5.4490; Train perplexity: 119.2483; Val perplexity: 232.5213\n",
      "Iter: 2000; Train loss: 3513.1409; Val loss: 5.4022; Train perplexity: 122.1739; Val perplexity: 221.9010\n",
      "Iter: 2040; Train loss: 3564.2883; Val loss: 5.3924; Train perplexity: 136.2370; Val perplexity: 219.7370\n",
      "Iter: 2080; Train loss: 3386.6650; Val loss: 5.4092; Train perplexity: 105.3334; Val perplexity: 223.4636\n",
      "Iter: 2120; Train loss: 3587.0286; Val loss: 5.4198; Train perplexity: 132.6383; Val perplexity: 225.8390\n",
      "Iter: 2160; Train loss: 3421.1458; Val loss: 5.4199; Train perplexity: 100.8214; Val perplexity: 225.8633\n",
      "Iter: 2200; Train loss: 3535.3312; Val loss: 5.4154; Train perplexity: 104.2730; Val perplexity: 224.8395\n",
      "Iter: 2240; Train loss: 3368.3983; Val loss: 5.4138; Train perplexity: 100.6092; Val perplexity: 224.4779\n",
      "Iter: 2280; Train loss: 3537.6441; Val loss: 5.4254; Train perplexity: 120.0659; Val perplexity: 227.1125\n",
      "Iter: 2320; Train loss: 3604.9661; Val loss: 5.4000; Train perplexity: 123.8200; Val perplexity: 221.3988\n",
      "Iter: 2360; Train loss: 3387.9238; Val loss: 5.4312; Train perplexity: 125.7372; Val perplexity: 228.4234\n",
      "Iter: 2400; Train loss: 3464.1634; Val loss: 5.4388; Train perplexity: 116.7577; Val perplexity: 230.1611\n",
      "Iter: 2440; Train loss: 3510.8679; Val loss: 5.4092; Train perplexity: 121.4552; Val perplexity: 223.4568\n",
      "Iter: 2480; Train loss: 3588.3339; Val loss: 5.4229; Train perplexity: 115.8673; Val perplexity: 226.5370\n",
      "Iter: 2520; Train loss: 3504.1345; Val loss: 5.4133; Train perplexity: 118.8388; Val perplexity: 224.3802\n",
      "Iter: 2560; Train loss: 3446.6526; Val loss: 5.4340; Train perplexity: 109.7574; Val perplexity: 229.0674\n",
      "Iter: 2600; Train loss: 3356.1333; Val loss: 5.4459; Train perplexity: 106.9638; Val perplexity: 231.8020\n",
      "Iter: 2640; Train loss: 3436.6079; Val loss: 5.4225; Train perplexity: 122.6255; Val perplexity: 226.4429\n",
      "Iter: 2680; Train loss: 3411.3875; Val loss: 5.4200; Train perplexity: 117.2148; Val perplexity: 225.8700\n",
      "Iter: 2720; Train loss: 3477.7050; Val loss: 5.4457; Train perplexity: 116.1852; Val perplexity: 231.7582\n",
      "Iter: 2760; Train loss: 3439.5279; Val loss: 5.4360; Train perplexity: 111.4548; Val perplexity: 229.5208\n",
      "Iter: 2800; Train loss: 3350.1558; Val loss: 5.4314; Train perplexity: 113.3358; Val perplexity: 228.4747\n",
      "Iter: 2840; Train loss: 3224.7126; Val loss: 5.4395; Train perplexity: 86.6369; Val perplexity: 230.3217\n",
      "Iter: 2880; Train loss: 3438.3102; Val loss: 5.4347; Train perplexity: 106.4814; Val perplexity: 229.2181\n",
      "Iter: 2920; Train loss: 3316.0232; Val loss: 5.4528; Train perplexity: 93.6480; Val perplexity: 233.4035\n",
      "Iter: 2960; Train loss: 3416.6757; Val loss: 5.4575; Train perplexity: 112.8455; Val perplexity: 234.5099\n",
      "Iter: 3000; Train loss: 3273.1221; Val loss: 5.4620; Train perplexity: 88.9463; Val perplexity: 235.5604\n",
      "Iter: 3040; Train loss: 3406.5590; Val loss: 5.4764; Train perplexity: 108.9905; Val perplexity: 238.9880\n",
      "Iter: 3080; Train loss: 3454.6728; Val loss: 5.4570; Train perplexity: 105.5174; Val perplexity: 234.3953\n",
      "Iter: 3120; Train loss: 3275.2179; Val loss: 5.4652; Train perplexity: 96.3921; Val perplexity: 236.3117\n",
      "Iter: 3160; Train loss: 3358.4692; Val loss: 5.4679; Train perplexity: 92.5514; Val perplexity: 236.9723\n",
      "Iter: 3200; Train loss: 3424.5670; Val loss: 5.4546; Train perplexity: 121.0022; Val perplexity: 233.8341\n",
      "Iter: 3240; Train loss: 3376.2870; Val loss: 5.4625; Train perplexity: 109.7740; Val perplexity: 235.6896\n",
      "Iter: 3280; Train loss: 3344.1724; Val loss: 5.4559; Train perplexity: 103.8964; Val perplexity: 234.1322\n",
      "Iter: 3320; Train loss: 3370.6523; Val loss: 5.4558; Train perplexity: 90.5515; Val perplexity: 234.1059\n",
      "Iter: 3360; Train loss: 3379.3188; Val loss: 5.4571; Train perplexity: 110.5453; Val perplexity: 234.4253\n",
      "Iter: 3400; Train loss: 3359.4802; Val loss: 5.4504; Train perplexity: 92.6211; Val perplexity: 232.8539\n",
      "Iter: 3440; Train loss: 3633.0787; Val loss: 5.4917; Train perplexity: 129.7798; Val perplexity: 242.6586\n",
      "Iter: 3480; Train loss: 3315.3920; Val loss: 5.4551; Train perplexity: 90.9749; Val perplexity: 233.9527\n",
      "Iter: 3520; Train loss: 3261.8644; Val loss: 5.4618; Train perplexity: 84.0515; Val perplexity: 235.5135\n",
      "Iter: 3560; Train loss: 3513.7248; Val loss: 5.4678; Train perplexity: 105.5664; Val perplexity: 236.9481\n",
      "Iter: 3600; Train loss: 3558.8876; Val loss: 5.4794; Train perplexity: 132.3205; Val perplexity: 239.6929\n",
      "Iter: 3640; Train loss: 3342.0522; Val loss: 5.4712; Train perplexity: 102.1864; Val perplexity: 237.7506\n",
      "Iter: 3680; Train loss: 3456.3527; Val loss: 5.4696; Train perplexity: 112.9151; Val perplexity: 237.3742\n",
      "Iter: 3720; Train loss: 3393.9380; Val loss: 5.4550; Train perplexity: 104.0236; Val perplexity: 233.9176\n",
      "Iter: 3760; Train loss: 3488.3997; Val loss: 5.4879; Train perplexity: 112.0378; Val perplexity: 241.7476\n",
      "Iter: 3800; Train loss: 3438.3078; Val loss: 5.4675; Train perplexity: 115.8534; Val perplexity: 236.8653\n",
      "Iter: 3840; Train loss: 3389.7532; Val loss: 5.4828; Train perplexity: 96.3750; Val perplexity: 240.5105\n",
      "Iter: 3880; Train loss: 3408.0977; Val loss: 5.4969; Train perplexity: 112.9823; Val perplexity: 243.9377\n",
      "Iter: 3920; Train loss: 3305.8130; Val loss: 5.4640; Train perplexity: 95.5438; Val perplexity: 236.0397\n",
      "Iter: 3960; Train loss: 3395.7972; Val loss: 5.4716; Train perplexity: 107.5776; Val perplexity: 237.8311\n",
      "Iter: 4000; Train loss: 3405.6465; Val loss: 5.4672; Train perplexity: 103.6758; Val perplexity: 236.7962\n"
     ]
    }
   ],
   "source": [
    "# Initialize print_loss for tracking progress\n",
    "print_loss = 0\n",
    "print_total_words = 0\n",
    "losses = []\n",
    "total_words = []\n",
    "loss_avgs = []\n",
    "perplexity_scores = []\n",
    "loss_avgs_val = []\n",
    "perplexity_scores_val = []\n",
    "    \n",
    "for iteration in range(1, n_iteration + 1):\n",
    "    training_batch = [random.choice(training_pairs) for _ in range(batch_size)]\n",
    "    \n",
    "    # Extract fields from batch\n",
    "    input_variable, lengths, target_variable, mask, max_target_len = batch2TrainData(voc, training_batch)\n",
    "    \n",
    "    # Run a training iteration\n",
    "    loss, n_total = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                 decoder, encoder_optimizer, decoder_optimizer, batch_size, clip, teacher_forcing_ratio)\n",
    "    \n",
    "    print_loss += loss\n",
    "    print_total_words += n_total\n",
    "    losses.append(loss)\n",
    "    total_words.append(n_total)\n",
    "\n",
    "    if iteration % validate_print_save == 0:\n",
    "        # Validation \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        validation_batches = [validation_pairs[i:i+batch_size] for i in range(0, len(validation_pairs), batch_size)][:-1]\n",
    "        n_validation = len(validation_batches)\n",
    "        validation_loss = 0\n",
    "        val_n_total = 0\n",
    "\n",
    "        for i in range(n_validation):\n",
    "            validation_batch = validation_batches[i]\n",
    "            input_variable, lengths, target_variable, mask, max_target_len = batch2TrainData(voc, validation_batch)\n",
    "            val_loss, n_total = validate(encoder, decoder, batch_size, input_variable, lengths, target_variable, mask, max_target_len)\n",
    "            validation_loss += val_loss\n",
    "            val_n_total += n_total\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        validation_loss_avg = validation_loss / val_n_total\n",
    "        perplexity_val = torch.exp(torch.tensor(validation_loss / val_n_total))\n",
    "        loss_avgs_val.append(validation_loss_avg)\n",
    "        perplexity_scores_val.append(perplexity_val)\n",
    "\n",
    "        # Print progress\n",
    "        print_loss_avg = print_loss / validate_print_save\n",
    "        perplexity = torch.exp(torch.tensor(print_loss / print_total_words))\n",
    "        loss_avgs.append(print_loss_avg)\n",
    "        perplexity_scores.append(perplexity)\n",
    "        print(f\"Iter: {iteration}; Train loss: {print_loss_avg:.4f}; Val loss: {validation_loss_avg:.4f}; Train perplexity: {perplexity:.4f}; Val perplexity: {perplexity_val:.4f}\")\n",
    "        print_loss = 0\n",
    "        print_total_words = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        directory = os.path.join(\"checkpoints\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        torch.save({\n",
    "            'iteration': iteration,\n",
    "            'en': encoder.state_dict(),\n",
    "            'de': decoder.state_dict(),\n",
    "            'en_opt': encoder_optimizer.state_dict(),\n",
    "            'de_opt': decoder_optimizer.state_dict(),\n",
    "            'train loss': loss_avgs,\n",
    "            'val loss': loss_avgs_val,\n",
    "            'train perplexity': perplexity_scores,\n",
    "            'val perplexity': perplexity_scores_val,\n",
    "            'voc_dict': voc.__dict__,\n",
    "        }, os.path.join(directory, f'{iteration}_checkpoint.tar'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
